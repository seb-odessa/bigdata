I. Pre-requirements

Use Docker to create hadoop cluster  with 2+ slave nodes
Implemetn 2 types of containers
1. Namenode + Resource manager
2. Datanode + Node manager + History server

Acceptance criteria:
1. Namenode web ui should expose 2+ connected datanodes
2. Put file in hdfs using cli and cat it after from a command line
3. Resource manager expose 2+ connected

Hints:

1. You have to specify static ip and hostnames for every container. Do not use docker compose (it doesn't provide a way to specify static ip for now)
2. If something doesn't work check logs of services and containers(applications)
3. HDFS uses hosts for communication between nodes

II. Extended payments job:

Implement job that uses input from Payment job(datasets/payments.log) and store fields: id, sum and list of hosts as a valid json

Example:

    Input data:

        2014-07-02 20:52:39 1 12.01 www.store1.com
        2014-07-02 20:52:39 1123 1.75 www.store1.com
        2014-07-02 20:52:39 12 4.05 www.store2.com
        2014-07-02 20:52:39 1 7.87 www.store1.com
        2014-07-02 20:52:40 12 124.67 www.store2.com
        2014-07-02 20:52:40 1 9.14 www.store3.com
        2014-07-02 20:52:40 1123 14.75 www.store1.com
        2014-07-02 20:52:40 12 54.95 www.store2.com
        2014-07-02 20:52:40 1 77.70 www.store3.com
        2014-07-02 20:52:40 12 1.99 www.store4.com

    Output data:

        {"id":1,"total":106.72,"stores":["www.store1.com","www.store3.com"]
        {"id":1123,"total":16.50,"stores":["www.store1.com"]}
        {"id":12,"total":185.66,"stores":["www.store2.com","www.store4.com"]}


III. Custom output format

Implement own output format for csv or json
Use it for exercise II

